{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib2\n",
    "from ptb import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and process data, utility functions\n",
    "\"\"\"\n",
    "\n",
    "file_url = \"https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\"\n",
    "file_name = \"data/tinyshakespeare.txt\"\n",
    "\n",
    "if not os.path.exists(\"data/\"):\n",
    "    os.makedirs(\"data/\")\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    response = urllib2.urlopen(file_url)\n",
    "\n",
    "    fh = open(file_name, \"w\")\n",
    "    fh.write(response.read())\n",
    "    fh.close()\n",
    "\n",
    "with open(file_name, \"r\") as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data\n",
    "\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if \"sess\" in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def train_network(g, num_epochs, num_steps=200, batch_size=32, verbose=True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            \n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "                feed_dict={g[\"x\"]:X, g[\"y\"]:Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g[\"init_state\"]] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g[\"total_loss\"],\n",
    "                                                              g[\"final_state\"],\n",
    "                                                              g[\"train_step\"]],\n",
    "                                                                feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \": \", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "        \n",
    "        if isinstance(save, str):\n",
    "            g[\"saver\"].save(sess, save)\n",
    "                    \n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_basic_rnn_graph_with_list(\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    learning_rate = 1e-4):\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_placeholder\")\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"output_placeholder\")\n",
    "\n",
    "    \"\"\"\n",
    "    RNN Inputs\n",
    "    \"\"\"\n",
    "    embeddings = tf.get_variable(\"embedding_matrix\", [num_classes, state_size])\n",
    "    rnn_inputs = [tf.squeeze(i) for i in tf.split(tf.nn.embedding_lookup(embeddings,x), num_steps, 1)]\n",
    "    \n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=state_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "    # logits and predictions\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W = tf.get_variable(\"W\", [state_size, num_classes])\n",
    "        b = tf.get_variable(\"b\", [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(y, num_steps, 1)]\n",
    "    \n",
    "    \n",
    "    # losses and train_step\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    return dict(x = x,\n",
    "                y = y,\n",
    "                init_state = init_state,\n",
    "                final_state = final_state,\n",
    "                total_loss = total_loss,\n",
    "                train_step = train_step\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "build_basic_rnn_graph_with_list()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_list(\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_placeholder\")\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"output_placeholder\")\n",
    "\n",
    "    \"\"\"\n",
    "    RNN Inputs\n",
    "    \"\"\"\n",
    "    embeddings = tf.get_variable(\"embedding_matrix\", [num_classes, state_size])\n",
    "    rnn_inputs = [tf.squeeze(i) for i in tf.split(tf.nn.embedding_lookup(embeddings,x), num_steps, 1)]\n",
    "\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=state_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    # logits and predictions\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W = tf.get_variable(\"W\", [state_size, num_classes])\n",
    "        b = tf.get_variable(\"b\", [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(y, num_steps, 1)]\n",
    "\n",
    "    # losses and train_step\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    return dict(x = x,\n",
    "                y = y,\n",
    "                init_state = init_state,\n",
    "                final_state = final_state,\n",
    "                total_loss = total_loss,\n",
    "                train_step = train_step\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "build_multilayer_lstm_graph_with_list()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_dynamic_rnn(\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [batch_size, None], name=\"input_placeholder\")\n",
    "    y = tf.placeholder(tf.int32, [batch_size, None], name=\"output_placeholder\")\n",
    "\n",
    "    \"\"\"\n",
    "    RNN Inputs\n",
    "    \"\"\"\n",
    "    embeddings = tf.get_variable(\"embedding_matrix\", [num_classes, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings,x)\n",
    "\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=state_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    # logits and predictions\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W = tf.get_variable(\"W\", [state_size, num_classes])\n",
    "        b = tf.get_variable(\"b\", [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped  = tf.reshape(y, [-1])\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    # losses and train_step\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_reshaped, logits = logits)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    return dict(x = x,\n",
    "                y = y,\n",
    "                init_state = init_state,\n",
    "                final_state = final_state,\n",
    "                total_loss = total_loss,\n",
    "                train_step = train_step\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.38076400757 seconds to build the graph\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "build_multilayer_lstm_graph_with_dynamic_rnn()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 :  3.5336919008\n",
      "Average training loss for Epoch 1 :  3.31842687471\n",
      "Average training loss for Epoch 2 :  3.31412000384\n",
      "It took 283.329195023 seconds to train for 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "g = build_multilayer_lstm_graph_with_dynamic_rnn()\n",
    "t = time.time()\n",
    "train_network(g,3)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 3 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln(tensor, scope=None, epsilon=1e-5):\n",
    "    \"\"\" Layer normalizes a 2D tensor along its second axis \"\"\"\n",
    "    assert(len(tensor.get_shape())==2)\n",
    "    m,v = tf.nn.moments(tensor, [1], keep_dims=True)\n",
    "    if not isinstance(scope, str):\n",
    "        scope = \"\"\n",
    "    with tf.variable_scope(scope + \"layer_norm\"):\n",
    "        scale = tf.get_variable(\"scale\", \n",
    "                                shape = [tensor.get_shape()[1]],\n",
    "                                initializer = tf.constant_initializer(1))\n",
    "        shift = tf.get_variable(\"shift\",\n",
    "                                shape = [tensor.get_shape()[1]],\n",
    "                                initializer = tf.constant_initializer(0))\n",
    "    \n",
    "    LN_initial = (tensor - m) / tf.sqrt(v + epsilon)\n",
    "    \n",
    "    return LN_initial * scale + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalizedLSTMCell(tf.contrib.rnn.BasicRNNCell):\n",
    "    \"\"\"\n",
    "    Adapted from TF's BasicRNNCell to use Layer Normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_units, forget_bias=1.0, activation=tf.nn.tanh):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.contrib.rnn.LSTMStateTuple(self._num_units, self._num_units)\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "    \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM)\"\"\"\n",
    "        c, h = state\n",
    "        \n",
    "        concat = tf.nn.rnn._linear([inputs, h], 4 * self._num_units, False)\n",
    "\n",
    "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "        i, j, f, o = tf.split(value=concat, num_or_size_splits=4, axis=1)\n",
    "        \n",
    "        # add layer normalization to each gate\n",
    "        i = ln(i, scope=\"i/\")\n",
    "        j = ln(j, scope=\"j/\")\n",
    "        f = ln(f, scope=\"f/\")\n",
    "        o = ln(o, scope=\"o/\")\n",
    "\n",
    "        new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) *\n",
    "                   self._activation(j))\n",
    "\n",
    "        # add layer_normalization in calculation of new hidden state\n",
    "        new_h = self._activation(ln(new_c, scope=\"new_h/\")) * tf.nn.sigmoid(o)\n",
    "        new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "        \n",
    "        return new_h, new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    cell_type = None,\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    build_with_dropout = False,\n",
    "    learning_rate = 1e-4):\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [batch_size, None], name=\"input_placeholder\")\n",
    "    y = tf.placeholder(tf.int32, [batch_size, None], name=\"labels_placeholder\")\n",
    "    \n",
    "    dropout = tf.constant(1.0)\n",
    "\n",
    "    \"\"\"\n",
    "    RNN Inputs\n",
    "    \"\"\"\n",
    "    embeddings = tf.get_variable(\"embedding_matrix\", [num_classes, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings,x)\n",
    "\n",
    "    if cell_type == \"GRU\":\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units=state_size)\n",
    "    elif cell_type == \"LSTM\":\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units=state_size)\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = LayerNormalizedLSTMCell(num_units=state_size)\n",
    "    else:\n",
    "        cell = tf.contrib.rnn.BasicRNNCell(num_units=state_size)\n",
    "        \n",
    "    if build_with_dropout:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = dropout)\n",
    "        \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = dropout)\n",
    "        \n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    # logits and predictions\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W = tf.get_variable(\"W\", [state_size, num_classes])\n",
    "        b = tf.get_variable(\"b\", [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped  = tf.reshape(y, [-1])\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    # losses and train_step\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_reshaped, logits = logits)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    return dict(x = x,\n",
    "                y = y,\n",
    "                init_state = init_state,\n",
    "                final_state = final_state,\n",
    "                total_loss = total_loss,\n",
    "                train_step = train_step,\n",
    "                preds = predictions,\n",
    "                saver = tf.train.Saver()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 :  3.35949899188\n",
      "Average training loss for Epoch 1 :  2.75694669769\n",
      "Average training loss for Epoch 2 :  2.43157069891\n",
      "Average training loss for Epoch 3 :  2.28761342523\n",
      "Average training loss for Epoch 4 :  2.19376960151\n",
      "Average training loss for Epoch 5 :  2.12189557629\n",
      "Average training loss for Epoch 6 :  2.06530556684\n",
      "Average training loss for Epoch 7 :  2.01816824817\n",
      "Average training loss for Epoch 8 :  1.97790918738\n",
      "Average training loss for Epoch 9 :  1.9427579953\n",
      "Average training loss for Epoch 10 :  1.91151862325\n",
      "Average training loss for Epoch 11 :  1.88353007266\n",
      "Average training loss for Epoch 12 :  1.85823383463\n",
      "Average training loss for Epoch 13 :  1.83507604227\n",
      "Average training loss for Epoch 14 :  1.81370611749\n",
      "Average training loss for Epoch 15 :  1.79390396984\n",
      "Average training loss for Epoch 16 :  1.77555054043\n",
      "Average training loss for Epoch 17 :  1.75848735445\n",
      "Average training loss for Epoch 18 :  1.74255770882\n",
      "Average training loss for Epoch 19 :  1.7276615413\n",
      "It took 7423.5930779 seconds to train for 20 epochs.\n",
      "The average loss on the final epoch was: 1.7276615413\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"saves/\"):\n",
    "    os.makedirs(\"saves/\")\n",
    "\n",
    "g = build_graph(cell_type=\"GRU\", num_steps=80)\n",
    "t = time.time()\n",
    "losses = train_network(g, 20, num_steps=80, save=\"saves/GRU_20_epochs\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state \"\"\"\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        g[\"saver\"].restore(sess, checkpoint)\n",
    "        \n",
    "        #state = None\n",
    "        state = sess.run([g[\"final_state\"]])\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "        \n",
    "        for i in range(num_chars):\n",
    "\n",
    "            #if state is not None:\n",
    "            feed_dict = {g[\"x\"]: [[current_char]], g[\"init_state\"]:state}\n",
    "            #else:\n",
    "            #    feed_dict = {g[\"x\"]: [[current_char]]}\n",
    "        \n",
    "            preds, state = sess.run([g[\"preds\"], g[\"final_state\"]], feed_dict)\n",
    "            \n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size,1,p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size,1,p=np.squeeze(preds))[0]\n",
    "            \n",
    "            chars.append(current_char)\n",
    "        \n",
    "    chars = map(lambda x:idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-dee3d0838fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GRU\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_characters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saves/GRU_20_epochs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpick_top_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-d8244f6aae3d>\u001b[0m in \u001b[0;36mgenerate_characters\u001b[0;34m(g, checkpoint, num_chars, prompt, pick_top_chars)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#    feed_dict = {g[\"x\"]: [[current_char]]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"final_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpick_top_chars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\n\u001b[0m\u001b[1;32m    926\u001b[0m                             \u001b[0;34m'Acceptable feed values include Python scalars, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                             'strings, lists, or numpy ndarrays.')\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays."
     ]
    }
   ],
   "source": [
    "g = build_graph(cell_type=\"GRU\", num_steps=1, batch_size=1)\n",
    "generate_characters(g, \"saves/GRU_20_epochs\", 750, prompt=\"A\", pick_top_chars=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'rnn/while/Exit_2:0' shape=(1, 100) dtype=float32>, <tf.Tensor 'rnn/while/Exit_3:0' shape=(1, 100) dtype=float32>, <tf.Tensor 'rnn/while/Exit_4:0' shape=(1, 100) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    g[\"saver\"].restore(sess, \"saves/GRU_20_epochs\")\n",
    "    print(g[\"final_state\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
